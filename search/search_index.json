{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"I am Ignacio Peletier a Spanish Telecommunications engineer specialized in Data Science. I am very passionate about my work and I enjoy freelancing data-related projects in my spare time. Constantly learning, currently I am immersed in topics such as causal inference and Bayesian statistics. Blog Although I will cover diverse topics in my blog, you will mainly find: statistical inference, causal inference, bayesian statistics and machine learning . Valorant Aim . Here you will find an analysis about accuracy with different guns while playing Valorant. Valorant Aim: The Bayesian Way . Using the same data as the previous post, we build a logistic regression the Bayesian way. Work Experience Senior Data Scientist at Busuu | July 2025 - Actual Senior Growth Data Scientist at MoonPay | December 2024 - July 2025 Senior Data Scientist at Busuu | October 2023 - December 2024 Senior Data Scientist at Cabify | May 2022 - October 2023 Data Scientist at Haivision | March 2020 - May 2022 Machine Learning Engineer at LightFlow (acquired by Haivison) | June 2019 - March 2020 Deep Learning and Blockchain Researcher at GAPS, UPM | September 2018 - June 2019 Deep Learning Engineer at Epic Labs | July, 2017 - July 2018 Researcher Microwave and Radar group, UPM | September 2015 - June 2017 Published Papers Video Tampering Detection for Decentralized Video Transcoding Networks | Springer, 2020 Materiality analysis in sustainability reporting: Insights from large Spanish companies | Wiley, 2024 Published Books En qu\u00e9 piensan los robots | Deusto, 2024","title":"Welcome"},{"location":"#blog","text":"Although I will cover diverse topics in my blog, you will mainly find: statistical inference, causal inference, bayesian statistics and machine learning . Valorant Aim . Here you will find an analysis about accuracy with different guns while playing Valorant. Valorant Aim: The Bayesian Way . Using the same data as the previous post, we build a logistic regression the Bayesian way.","title":"Blog"},{"location":"#work-experience","text":"Senior Data Scientist at Busuu | July 2025 - Actual Senior Growth Data Scientist at MoonPay | December 2024 - July 2025 Senior Data Scientist at Busuu | October 2023 - December 2024 Senior Data Scientist at Cabify | May 2022 - October 2023 Data Scientist at Haivision | March 2020 - May 2022 Machine Learning Engineer at LightFlow (acquired by Haivison) | June 2019 - March 2020 Deep Learning and Blockchain Researcher at GAPS, UPM | September 2018 - June 2019 Deep Learning Engineer at Epic Labs | July, 2017 - July 2018 Researcher Microwave and Radar group, UPM | September 2015 - June 2017","title":"Work Experience"},{"location":"#published-papers","text":"Video Tampering Detection for Decentralized Video Transcoding Networks | Springer, 2020 Materiality analysis in sustainability reporting: Insights from large Spanish companies | Wiley, 2024","title":"Published Papers"},{"location":"#published-books","text":"En qu\u00e9 piensan los robots | Deusto, 2024","title":"Published Books"},{"location":"bayesian_valorant_aim/","text":"After studying Bayesian Statistics for a while, I revisited an experiment I conducted using VALORANT data. The experimental setup was: 30 bots were configured in the Range. They were static, had armor (150 HP), and the difficulty was set to medium. Shots were fired at two distances: close and long . Three weapons were used: Sheriff, Phantom, and Vandal . For each configuration, 10 measurements were taken: 3 weapons \u00d7 2 distances \u00d7 10 measurements = 60 samples . The order was randomized to reduce bias. Methodology Unlike the previous analysis that focused on specific questions, here the goal is to work with a generative model . This allows us to answer questions while properly quantifying uncertainty using simulations, which are central to Bayesian statistics. The computations are based on a simple causal model. The Data After importing libraries and preparing the data, we obtain the following sample: weapon distance bots prop round distance_weapon 2 2 18 0.600000 1 4 3 2 19 0.633333 2 5 3 1 26 0.866667 3 2 1 1 16 0.533333 4 0 3 1 23 0.766667 5 2 Relevant columns: distance_weapon : indicator (0\u20132 = close with Sheriff, Phantom, Vandal; 3\u20135 = long with same order). round : round number (1\u201360). bots : number of bots downed (0\u201330). Causal Thinking The following DAG (Directed Acyclic Graph) describes the data-generating process. We expect the number of downed bots to be influenced by: Weapon \u2013 Sheriff (pistol), Phantom (faster fire, lower damage), Vandal (higher damage). Distance \u2013 accuracy decreases with longer distance. Round \u2013 aim may improve with practice. Since the experiment was randomized, we assume no additional associations. Thinking causally helps communicate assumptions. Not specifying a causal structure is still imposing one implicitly, often incorrectly. Starting simple and adding complexity is usually best. The Model We build a binomial regression, with a logistic link: log(p/(1-p)) = beta_i + round\u00b7j Where beta_i is different for each combination of distance and weapon ( i ). round is the coefficient associated with the learning as the rounds progressed. j is the round but normalized to the maximum rounds played. An uninformative prior is used. Model structure in PyMC : Sampling Using uninformative priors, the prior expectation is extreme (all 0 or all 30 bots), which motivates choosing better priors in the future: Posterior traces look good (no strong correlation between samples): Parameter summary: mean sd hdi_2.5% hdi_97.5% r_hat beta[0] 0.35 0.15 0.06 0.63 1.0 beta[1] 0.45 0.17 0.12 0.79 1.0 beta[2] 1.33 0.17 1.00 1.66 1.0 beta[3] 0.12 0.14 -0.14 0.40 1.0 beta[4] 0.10 0.14 -0.16 0.37 1.0 beta[5] 0.80 0.16 0.47 1.11 1.0 round 0.33 0.19 -0.03 0.69 1.0 All r_hat=1 , indicating convergence. Instead of focusing only on means, Bayesian models allow exploring full distributions and their uncertainty. Comparison of observed data vs. posterior distribution: Cumulative distribution: Parameters Plotting the betas (after logistic transform): Interpretation: Accuracy decreases at long distance. Vandal accuracy is higher than Phantom or Sheriff. Phantom and Sheriff are similar. Unlike before (bot counts), this reflects accuracy probabilities , which are more intrinsic. Round In the previous analysis, we removed this variable from the model after testing its significance. What we will be doing now is averaging our accuracy from different weapons and distances in order to: Measure the improvement in accuracy after playing 60 rounds, both in p.p. and %. See how this change in accuracy translates to how the number of downed bots changes. The starting accuracy is: 63%, after playing 60 rounds, the accuracy was: 70% Numbers are nice, but let's work with distributions to better measure uncertainty. We do this in p.p. (percentage points): And in relative increase (%): After training, we observe an overall increase in accuracy, 12% on average. Most of the density is positive, but there is significant uncertainty: 95% of the probability mass lies between -2.2% and +25% . Now, let\u2019s see how this increase in precision translates to the number of downed bots. We do this by simulating rounds with the precisions before and after training. Since we are not using averages but whole distributions, the uncertainty in our p \u2019s is carried over to the number of downed bots. This was not done in the previous analysis, since the confidence intervals there referred to the mean of the distribution, not the full distribution. Although the average difference is positive, there is substantial uncertainty in how many additional bots we expect to down after training. Since we have the full distribution, we can ask questions such as: After playing 60 rounds, what is the probability of downing more bots than without training? The probability is 67% After playing 60 rounds, what is the probability of downing 5 or more bots than without training? The probability is 28% Querying a model like this is just awesome! Distance We now take a look at the effect of distance. We average precision across weapons and check how much accuracy is lost as we move further from the target. This is the decrease in accuracy in percentage points: These absolute differences translate to the following relative difference (%): At long distance, accuracy decreases by 13% on average, and this difference is clearly negative. The 95% HDI ranges between 6.6% and 19% . As done before, we can query our model and check how much probability mass lies below zero: The area below zero is 99.975% The model strongly suggests that the effect of distance is negative. But knowing the decrease in accuracy is one thing\u2014what does this decrease, and its uncertainty, mean for the number of downed bots? Even though the effect of distance is mostly negative, the difference in downed bots is not so straightforward: the 95% HDI is between -11 and 4 . This may surprise the reader, but the following exercise clarifies that due to randomness in the process, higher precision does not always guarantee more downed bots. Let\u2019s run a simulation where Player 1 has p=0.8 and Player 2 has p=0.7 . Player 1 is more accurate, but if they play 1000 rounds, will Player 1 always down more bots than Player 2? I encourage the reader to think before looking at the results. Player 1 had more downed bots than Player 2 in 790 out of 1000 rounds. The 95% HDI of the difference is [-4, 9] and the mean is 3.0 bots. Aha! Player 1 is not always the winner, even with +10 p.p. accuracy. Hopefully this sheds light on the results obtained earlier. Note that, even though both players\u2019 accuracies were fixed numbers, there is still uncertainty in the outcome due to the data-generating process. In our earlier model, when checking for differences, we used the whole posterior distribution of accuracies (at close and long distances), not just their mean values. This is crucial, since our estimate of the difference in downed bots reflects not only randomness in the process but also the uncertainty in our estimates of precision! Weapons: Vandal vs Phantom Last but not least, let\u2019s compare the Phantom and the Vandal. In terms of downing bots, here are the relative differences: Overall, we see that accuracy with the Vandal is 31% higher than with the Phantom. All of the probability mass is above 0%, and the 95% HDI is between 19% and 44% . This difference in accuracy translates to the following difference in downed bots: On average, we should expect to down 5.2 more bots with the Vandal. The probability of downing fewer bots with the Vandal than with the Phantom is: 6.4% We finish the analysis by showing the distribution of downed bots with the Phantom and the Vandal at both close and long distances:","title":"Valorant Aim: The Bayesian Way"},{"location":"bayesian_valorant_aim/#methodology","text":"Unlike the previous analysis that focused on specific questions, here the goal is to work with a generative model . This allows us to answer questions while properly quantifying uncertainty using simulations, which are central to Bayesian statistics. The computations are based on a simple causal model.","title":"Methodology"},{"location":"bayesian_valorant_aim/#the-data","text":"After importing libraries and preparing the data, we obtain the following sample: weapon distance bots prop round distance_weapon 2 2 18 0.600000 1 4 3 2 19 0.633333 2 5 3 1 26 0.866667 3 2 1 1 16 0.533333 4 0 3 1 23 0.766667 5 2 Relevant columns: distance_weapon : indicator (0\u20132 = close with Sheriff, Phantom, Vandal; 3\u20135 = long with same order). round : round number (1\u201360). bots : number of bots downed (0\u201330).","title":"The Data"},{"location":"bayesian_valorant_aim/#causal-thinking","text":"The following DAG (Directed Acyclic Graph) describes the data-generating process. We expect the number of downed bots to be influenced by: Weapon \u2013 Sheriff (pistol), Phantom (faster fire, lower damage), Vandal (higher damage). Distance \u2013 accuracy decreases with longer distance. Round \u2013 aim may improve with practice. Since the experiment was randomized, we assume no additional associations. Thinking causally helps communicate assumptions. Not specifying a causal structure is still imposing one implicitly, often incorrectly. Starting simple and adding complexity is usually best.","title":"Causal Thinking"},{"location":"bayesian_valorant_aim/#the-model","text":"We build a binomial regression, with a logistic link: log(p/(1-p)) = beta_i + round\u00b7j Where beta_i is different for each combination of distance and weapon ( i ). round is the coefficient associated with the learning as the rounds progressed. j is the round but normalized to the maximum rounds played. An uninformative prior is used. Model structure in PyMC :","title":"The Model"},{"location":"bayesian_valorant_aim/#sampling","text":"Using uninformative priors, the prior expectation is extreme (all 0 or all 30 bots), which motivates choosing better priors in the future: Posterior traces look good (no strong correlation between samples): Parameter summary: mean sd hdi_2.5% hdi_97.5% r_hat beta[0] 0.35 0.15 0.06 0.63 1.0 beta[1] 0.45 0.17 0.12 0.79 1.0 beta[2] 1.33 0.17 1.00 1.66 1.0 beta[3] 0.12 0.14 -0.14 0.40 1.0 beta[4] 0.10 0.14 -0.16 0.37 1.0 beta[5] 0.80 0.16 0.47 1.11 1.0 round 0.33 0.19 -0.03 0.69 1.0 All r_hat=1 , indicating convergence. Instead of focusing only on means, Bayesian models allow exploring full distributions and their uncertainty. Comparison of observed data vs. posterior distribution: Cumulative distribution:","title":"Sampling"},{"location":"bayesian_valorant_aim/#parameters","text":"Plotting the betas (after logistic transform): Interpretation: Accuracy decreases at long distance. Vandal accuracy is higher than Phantom or Sheriff. Phantom and Sheriff are similar. Unlike before (bot counts), this reflects accuracy probabilities , which are more intrinsic.","title":"Parameters"},{"location":"bayesian_valorant_aim/#round","text":"In the previous analysis, we removed this variable from the model after testing its significance. What we will be doing now is averaging our accuracy from different weapons and distances in order to: Measure the improvement in accuracy after playing 60 rounds, both in p.p. and %. See how this change in accuracy translates to how the number of downed bots changes. The starting accuracy is: 63%, after playing 60 rounds, the accuracy was: 70% Numbers are nice, but let's work with distributions to better measure uncertainty. We do this in p.p. (percentage points): And in relative increase (%): After training, we observe an overall increase in accuracy, 12% on average. Most of the density is positive, but there is significant uncertainty: 95% of the probability mass lies between -2.2% and +25% . Now, let\u2019s see how this increase in precision translates to the number of downed bots. We do this by simulating rounds with the precisions before and after training. Since we are not using averages but whole distributions, the uncertainty in our p \u2019s is carried over to the number of downed bots. This was not done in the previous analysis, since the confidence intervals there referred to the mean of the distribution, not the full distribution. Although the average difference is positive, there is substantial uncertainty in how many additional bots we expect to down after training. Since we have the full distribution, we can ask questions such as: After playing 60 rounds, what is the probability of downing more bots than without training? The probability is 67% After playing 60 rounds, what is the probability of downing 5 or more bots than without training? The probability is 28% Querying a model like this is just awesome!","title":"Round"},{"location":"bayesian_valorant_aim/#distance","text":"We now take a look at the effect of distance. We average precision across weapons and check how much accuracy is lost as we move further from the target. This is the decrease in accuracy in percentage points: These absolute differences translate to the following relative difference (%): At long distance, accuracy decreases by 13% on average, and this difference is clearly negative. The 95% HDI ranges between 6.6% and 19% . As done before, we can query our model and check how much probability mass lies below zero: The area below zero is 99.975% The model strongly suggests that the effect of distance is negative. But knowing the decrease in accuracy is one thing\u2014what does this decrease, and its uncertainty, mean for the number of downed bots? Even though the effect of distance is mostly negative, the difference in downed bots is not so straightforward: the 95% HDI is between -11 and 4 . This may surprise the reader, but the following exercise clarifies that due to randomness in the process, higher precision does not always guarantee more downed bots. Let\u2019s run a simulation where Player 1 has p=0.8 and Player 2 has p=0.7 . Player 1 is more accurate, but if they play 1000 rounds, will Player 1 always down more bots than Player 2? I encourage the reader to think before looking at the results. Player 1 had more downed bots than Player 2 in 790 out of 1000 rounds. The 95% HDI of the difference is [-4, 9] and the mean is 3.0 bots. Aha! Player 1 is not always the winner, even with +10 p.p. accuracy. Hopefully this sheds light on the results obtained earlier. Note that, even though both players\u2019 accuracies were fixed numbers, there is still uncertainty in the outcome due to the data-generating process. In our earlier model, when checking for differences, we used the whole posterior distribution of accuracies (at close and long distances), not just their mean values. This is crucial, since our estimate of the difference in downed bots reflects not only randomness in the process but also the uncertainty in our estimates of precision!","title":"Distance"},{"location":"bayesian_valorant_aim/#weapons-vandal-vs-phantom","text":"Last but not least, let\u2019s compare the Phantom and the Vandal. In terms of downing bots, here are the relative differences: Overall, we see that accuracy with the Vandal is 31% higher than with the Phantom. All of the probability mass is above 0%, and the 95% HDI is between 19% and 44% . This difference in accuracy translates to the following difference in downed bots: On average, we should expect to down 5.2 more bots with the Vandal. The probability of downing fewer bots with the Vandal than with the Phantom is: 6.4% We finish the analysis by showing the distribution of downed bots with the Phantom and the Vandal at both close and long distances:","title":"Weapons: Vandal vs Phantom"},{"location":"book/","text":"\ud83d\udcd8 En qu\u00e9 piensan los robots \u2014 An Artificial Intelligence Book for Everyone Artificial Intelligence is no longer just a theme from science fiction movies \u2014 it is shaping our everyday lives. From recommendation algorithms and delivery apps to medical robots and virtual assistants like ChatGPT, AI has become a daily presence . But with these innovations come urgent ethical questions: How do we prevent algorithms from reinforcing bias and discrimination? Who is accountable if an AI system makes a mistake in healthcare? Should we trust the future of humanity to a small group of tech executives and investors? Why This AI Book Matters En qu\u00e9 piensan los robots is written for anyone who wants to understand artificial intelligence and machine learning without needing a technical background. It explores both the opportunities and the challenges of AI, offering readers clear insights into: The fundamentals of AI and machine learning Real-world applications of algorithms in business and society The ethical dilemmas and responsibilities of building and using AI How each of us can take part in shaping a responsible AI future Instead of treating AI as something distant or overly technical, this book brings the debate closer to home. It provides the tools to understand how algorithms work, what they mean for our lives, and why ethical AI is everyone\u2019s responsibility . \ud83d\udc49 Get your copy now on Amazon . Press Coverage RTVE Radio 5 \u2013 Interview with Ignacio Peletier (Jan 30, 2025) Ignacio Peletier talks about the present and future of robots, highlighting how they are becoming increasingly integrated into daily life. Listen here RNE \u2013 La Cuadratura del C\u00edrculo \u2013 Do robots think like humans? This episode discusses how artificial neural networks are inspired (or not) by the human brain, featuring Ignacio Peletier and Miguel Serrano. Listen here Cadena SER \u2013 \"AI won't take your job\u2026\" Miguel Serrano explains that it's not AI that will take jobs, but the people who use it, discussing ethical challenges, machine creativity, and digital literacy. Listen or read more ComputerWorld \u2013 \"It's very biased to say that artificial intelligence will take our jobs\" ComputerWorld features a critical reflection on the idea that AI will completely replace humans. Read the article RNE \u2013 La Cuadratura del C\u00edrculo \u2013 When AI became afraid This episode addresses philosophical dilemmas about AI, its potential threat to humanity, and ethical concerns, with insights from Miguel Serrano. Listen here COPE \u2013 Imagina 2025 A New Year special exploring future scenarios in technology, economy, and society, mentioning the relevance of the book. Listen here YouTube \u2013 Fundaci\u00f3n Abante Video A video discussing topics related to the book and AI debates. Watch video","title":"About My Book"},{"location":"book/#en-que-piensan-los-robots-an-artificial-intelligence-book-for-everyone","text":"Artificial Intelligence is no longer just a theme from science fiction movies \u2014 it is shaping our everyday lives. From recommendation algorithms and delivery apps to medical robots and virtual assistants like ChatGPT, AI has become a daily presence . But with these innovations come urgent ethical questions: How do we prevent algorithms from reinforcing bias and discrimination? Who is accountable if an AI system makes a mistake in healthcare? Should we trust the future of humanity to a small group of tech executives and investors?","title":"\ud83d\udcd8 En qu\u00e9 piensan los robots \u2014 An Artificial Intelligence Book for Everyone"},{"location":"book/#why-this-ai-book-matters","text":"En qu\u00e9 piensan los robots is written for anyone who wants to understand artificial intelligence and machine learning without needing a technical background. It explores both the opportunities and the challenges of AI, offering readers clear insights into: The fundamentals of AI and machine learning Real-world applications of algorithms in business and society The ethical dilemmas and responsibilities of building and using AI How each of us can take part in shaping a responsible AI future Instead of treating AI as something distant or overly technical, this book brings the debate closer to home. It provides the tools to understand how algorithms work, what they mean for our lives, and why ethical AI is everyone\u2019s responsibility . \ud83d\udc49 Get your copy now on Amazon .","title":"Why This AI Book Matters"},{"location":"book/#press-coverage","text":"RTVE Radio 5 \u2013 Interview with Ignacio Peletier (Jan 30, 2025) Ignacio Peletier talks about the present and future of robots, highlighting how they are becoming increasingly integrated into daily life. Listen here RNE \u2013 La Cuadratura del C\u00edrculo \u2013 Do robots think like humans? This episode discusses how artificial neural networks are inspired (or not) by the human brain, featuring Ignacio Peletier and Miguel Serrano. Listen here Cadena SER \u2013 \"AI won't take your job\u2026\" Miguel Serrano explains that it's not AI that will take jobs, but the people who use it, discussing ethical challenges, machine creativity, and digital literacy. Listen or read more ComputerWorld \u2013 \"It's very biased to say that artificial intelligence will take our jobs\" ComputerWorld features a critical reflection on the idea that AI will completely replace humans. Read the article RNE \u2013 La Cuadratura del C\u00edrculo \u2013 When AI became afraid This episode addresses philosophical dilemmas about AI, its potential threat to humanity, and ethical concerns, with insights from Miguel Serrano. Listen here COPE \u2013 Imagina 2025 A New Year special exploring future scenarios in technology, economy, and society, mentioning the relevance of the book. Listen here YouTube \u2013 Fundaci\u00f3n Abante Video A video discussing topics related to the book and AI debates. Watch video","title":"Press Coverage"},{"location":"curriculum/","text":"I have experience in topics related to machine learning and statistics. My work spans from projects on image classification and object detection to optimizing business metrics using machine learning and measuring results with causal inference techniques, both in experimentation frameworks and with observational data. Starting from hardware in my uni, I switched to the software world in my first job at Epic Labs. After working there with deep learning for a year, I decided to strengthen my knowledge by pursuing a master's degree, during which I also joined a research group working on fraud detection with deep learning and blockchain. After completing my master\u2019s, I returned to Epic Labs, where I worked on their product Lightflow, which was later acquired by Haivision. There I contributed to machine learning algorithms for video optimization and data streaming. I then joined Cabify as a Senior Data Scientist in the pricing team, where I led initiatives on dynamic pricing strategies with causal inference, and behavioral economics, helping to improve marketplace efficiency and revenue. Later, I moved to Busuu, where I helped built the experimentation engine, and worked on user LTV modeling, and features powered by Large Language Models and Automatic Speech Recognition to enhance the learning experience. After a period at MoonPay as a Senior Growth Data Scientist in the monetization team, where I defined and optimized pricing strategies and applied advanced causal inference techniques, I returned to Busuu. Currently, I am working at Busuu on features that leverage statistics and Generative AI to better understand and correct learners\u2019 mistakes, with the goal of improving the personalization and effectiveness of the learning process. Work Experience Senior Data Scientist at Busuu | July 2025 - Actual Senior Growth Data Scientist at MoonPay | December 2024 - July 2025 Senior Data Scientist at Busuu | October 2023 - December 2024 Senior Data Scientist at Cabify | May 2022 - October 2023 Data Scientist at Haivision | March 2020 - May 2022 Machine Learning Engineer at LightFlow (acquired by Haivison) | June 2019 - March 2020 Deep Learning and Blockchain Researcher at GAPS, UPM | September 2018 - June 2019 Deep Learning Engineer at Epic Labs | July, 2017 - July 2018 Researcher Microwave and Radar group, UPM | September 2015 - June 2017 Education Master in Signal Processing and Machine Learning for Big Data, UPM Madrid, Spain | 2018 - 2019 Electrical Engineering and Computer Science degree, UPM Madrid, Spain | 2013 - 2017 Published Papers Video Tampering Detection for Decentralized Video Transcoding Networks | Springer, 2020 Materiality analysis in sustainability reporting: Insights from large Spanish companies | Wiley, 2024 Published Books En qu\u00e9 piensan los robots | Deusto, 2024 Complementary Education I have extended my education studying: Causal Inference in Statistics: A primer Statistical Rethinking Causal Inference and Discovery in Python Generalized Additive Models: an introduction with R And doing in Coursera: Crash Course in Causality: Inferring Causal Effects from Observational Data Essential Causal Inference Techniques for Data Science Statistical Modeling for Data Science Applications Specialization : Modern Regression Analysis in R ANOVA and Experimental Design Generalized Linear Models and Nonparametric Regression Practical Time Series Analysis Practical Machine Learning Regression Models Statistical Inference Bayesian Statistics: From Concept to Data Analysis","title":"Curriculum"},{"location":"curriculum/#work-experience","text":"Senior Data Scientist at Busuu | July 2025 - Actual Senior Growth Data Scientist at MoonPay | December 2024 - July 2025 Senior Data Scientist at Busuu | October 2023 - December 2024 Senior Data Scientist at Cabify | May 2022 - October 2023 Data Scientist at Haivision | March 2020 - May 2022 Machine Learning Engineer at LightFlow (acquired by Haivison) | June 2019 - March 2020 Deep Learning and Blockchain Researcher at GAPS, UPM | September 2018 - June 2019 Deep Learning Engineer at Epic Labs | July, 2017 - July 2018 Researcher Microwave and Radar group, UPM | September 2015 - June 2017","title":"Work Experience"},{"location":"curriculum/#education","text":"Master in Signal Processing and Machine Learning for Big Data, UPM Madrid, Spain | 2018 - 2019 Electrical Engineering and Computer Science degree, UPM Madrid, Spain | 2013 - 2017","title":"Education"},{"location":"curriculum/#published-papers","text":"Video Tampering Detection for Decentralized Video Transcoding Networks | Springer, 2020 Materiality analysis in sustainability reporting: Insights from large Spanish companies | Wiley, 2024","title":"Published Papers"},{"location":"curriculum/#published-books","text":"En qu\u00e9 piensan los robots | Deusto, 2024","title":"Published Books"},{"location":"curriculum/#complementary-education","text":"I have extended my education studying: Causal Inference in Statistics: A primer Statistical Rethinking Causal Inference and Discovery in Python Generalized Additive Models: an introduction with R And doing in Coursera: Crash Course in Causality: Inferring Causal Effects from Observational Data Essential Causal Inference Techniques for Data Science Statistical Modeling for Data Science Applications Specialization : Modern Regression Analysis in R ANOVA and Experimental Design Generalized Linear Models and Nonparametric Regression Practical Time Series Analysis Practical Machine Learning Regression Models Statistical Inference Bayesian Statistics: From Concept to Data Analysis","title":"Complementary Education"},{"location":"freelance/","text":"In my spare time, I enjoy freelancing on data-related projects. Some examples of my work include: Experimentation Consultancy \u2013 Designed and analyzed experiments to measure metric uplift for a mobile game. Covid-19 Impact Analysis \u2013 Quantified the effect of the pandemic on unemployment rates. Health App Study Design \u2013 Developed a framework for an observational study to evaluate app effectiveness. Marketing Mix Modeling \u2013 Built a MMM using Robyn . Sports Science Research \u2013 Created a statistical method to measure air resistance in cycling without the need for a wind tunnel. Healthcare Analytics \u2013 Assessed the impact of different treatments on newborn outcomes. Game Analytics \u2013 Developed a simulator to model in-game economy and racing performance. Contact \ud83d\udce7 Email : ignacio@peletier.com \ud83d\udcbc Upwork : My Freelancer Profile \ud83d\udd17 LinkedIn : My Profile","title":"Freelance"},{"location":"freelance/#contact","text":"\ud83d\udce7 Email : ignacio@peletier.com \ud83d\udcbc Upwork : My Freelancer Profile \ud83d\udd17 LinkedIn : My Profile","title":"Contact"},{"location":"valorant_aim/","text":"VALORANT is a free-to-play tactical first-person shooter by Riot Games. It launched in June 2020. I designed a small experiment to analyze aim performance. The study covers weapon accuracy, distance effects, and player improvement over time. Key questions: Which weapon gives me better results? Does distance reduce the number of bots I can eliminate? Do weapons perform differently at close vs. long range? Do I improve as the rounds progress? Experiment Setup In the practice range, 30 static bots were used. Each had armor (150 HP). Difficulty was set to medium . Two distances were tested: Close distance Long distance Weapons: Sheriff, Phantom, and Vandal. For every configuration, I recorded 10 trials: 3 weapons x 2 distances x 10 measurements = 60 samples . The order was randomized to reduce bias. Data Visualization The distribution of bots eliminated per weapon and distance: Distribution of downed bots in different setups Results show clear differences. The Vandal outperformed the Sheriff and Phantom. Later analysis checks if these differences are statistically significant. Weapon \u00d7 distance interaction was also tested. Does accuracy drop less for some weapons at longer range? Weapon \u00d7 distance interaction Findings: accuracy dropped at long range. The vandal is better at both distances. The Phantom and Sheriff interaction is interesting: the Phantom was stronger at close range but both are similar at longer range. I also tracked accuracy over time: Performance across rounds Later tests check if this trend was statistically different from zero. Analysis I used a two-way ANCOVA (Analysis of Covariance) with weapon , distance , and round . Linearity Assumption The linearity is assessed by visual inspection of the covariate ( round ) for each available group: Linearity assumption plot The smoothing method used was loess (locally weighted smoothing). The plots are more or less linear, with some exceptions. Homogeneity of Regression Slopes In order to proceed with the ANCOVA, it is important to check that there is no interaction between the covariate and each of the grouping variables. That is, it should be checked that the slope in the previous shown plot is the same for each group. The p-value obtained is 0.59 which indicates that this interaction is not statistically significant. Normality of Residuals The Shapiro-Wilk Normality Test is run in the residuals of the model producing a p-value of 0.937 which is not significant and thus the normality of the residuals is assumed. Homogeneity of Variances Levene's test is used to check that the variance of the residuals are equal between all the groups. The test yields a p-value of 0.482 which is again not significant and homogeneity of the residual variances is also assumed. ANCOVA Results ANOVA table: Effect DFn DFd F p p < 0.05 round 1 53 2.408 0.127 NO weapon 2 53 21.982 1.12e-07 YES distance 1 53 10.568 0.002 YES weapon:distance 2 53 0.202 0.818 NO Findings: No improvement over rounds. No interaction between weapon and distance. * Strong main effects of both weapon and distance . An F test is run with the previous model and a reduced one, which has no interaction term and round is removed. The test yields a non significant p-value of 0.4306 , with statistic F(53, 56) = 0.9345 , meaning that both models are statistically the same. Pairwise Comparisons (Tukey HSD) Using the previous reduced model, a Tukey HSD (Honest Significant Difference) test is run, in order to quantify the differences between the groups. Here are the results of the test: The differences between the Phantom and the Sheriff are not significant. (p-value of 0.8645 ). The Vandal produced an average of 5.30 more bots downed than the Sheriff. This value is significant (p-value = 0.0000004 ). The 95% Confidence Interval for this parameter is ( 3.1954, 7.4046 ). The Vandal produced an average of 4.85 more bots downed than the Phantom. This value is significant (p-value = 0.0000024 ). The 95% CI for this parameter is ( 2.7454, 6.9546 ). At long distance, the average of bots downed was 2.5667 less than at close distance. This value is significant (p-value = 0.0006838 ). The 95% CI for this parameter is ( 1.1368, 3.9965 ). Conclusions Vandal performed best , significantly above Sheriff and Phantom. Sheriff and Phantom were similar , despite weapon differences. Accuracy loss at long distance was consistent across weapons. No significant performance improvement across rounds. Disclaimer This dataset is from a single player (me). Results may not generalize, but the methodology can be reproduced. Shooting bots is not the same as competing against real players. All code is on GitHub . Clips of me playing are on YouTube .","title":"Valorant Aim"},{"location":"valorant_aim/#experiment-setup","text":"In the practice range, 30 static bots were used. Each had armor (150 HP). Difficulty was set to medium . Two distances were tested: Close distance Long distance Weapons: Sheriff, Phantom, and Vandal. For every configuration, I recorded 10 trials: 3 weapons x 2 distances x 10 measurements = 60 samples . The order was randomized to reduce bias.","title":"Experiment Setup"},{"location":"valorant_aim/#data-visualization","text":"The distribution of bots eliminated per weapon and distance: Distribution of downed bots in different setups Results show clear differences. The Vandal outperformed the Sheriff and Phantom. Later analysis checks if these differences are statistically significant. Weapon \u00d7 distance interaction was also tested. Does accuracy drop less for some weapons at longer range? Weapon \u00d7 distance interaction Findings: accuracy dropped at long range. The vandal is better at both distances. The Phantom and Sheriff interaction is interesting: the Phantom was stronger at close range but both are similar at longer range. I also tracked accuracy over time: Performance across rounds Later tests check if this trend was statistically different from zero.","title":"Data Visualization"},{"location":"valorant_aim/#analysis","text":"I used a two-way ANCOVA (Analysis of Covariance) with weapon , distance , and round .","title":"Analysis"},{"location":"valorant_aim/#linearity-assumption","text":"The linearity is assessed by visual inspection of the covariate ( round ) for each available group: Linearity assumption plot The smoothing method used was loess (locally weighted smoothing). The plots are more or less linear, with some exceptions.","title":"Linearity Assumption"},{"location":"valorant_aim/#homogeneity-of-regression-slopes","text":"In order to proceed with the ANCOVA, it is important to check that there is no interaction between the covariate and each of the grouping variables. That is, it should be checked that the slope in the previous shown plot is the same for each group. The p-value obtained is 0.59 which indicates that this interaction is not statistically significant.","title":"Homogeneity of Regression Slopes"},{"location":"valorant_aim/#normality-of-residuals","text":"The Shapiro-Wilk Normality Test is run in the residuals of the model producing a p-value of 0.937 which is not significant and thus the normality of the residuals is assumed.","title":"Normality of Residuals"},{"location":"valorant_aim/#homogeneity-of-variances","text":"Levene's test is used to check that the variance of the residuals are equal between all the groups. The test yields a p-value of 0.482 which is again not significant and homogeneity of the residual variances is also assumed.","title":"Homogeneity of Variances"},{"location":"valorant_aim/#ancova-results","text":"ANOVA table: Effect DFn DFd F p p < 0.05 round 1 53 2.408 0.127 NO weapon 2 53 21.982 1.12e-07 YES distance 1 53 10.568 0.002 YES weapon:distance 2 53 0.202 0.818 NO Findings: No improvement over rounds. No interaction between weapon and distance. * Strong main effects of both weapon and distance . An F test is run with the previous model and a reduced one, which has no interaction term and round is removed. The test yields a non significant p-value of 0.4306 , with statistic F(53, 56) = 0.9345 , meaning that both models are statistically the same.","title":"ANCOVA Results"},{"location":"valorant_aim/#pairwise-comparisons-tukey-hsd","text":"Using the previous reduced model, a Tukey HSD (Honest Significant Difference) test is run, in order to quantify the differences between the groups. Here are the results of the test: The differences between the Phantom and the Sheriff are not significant. (p-value of 0.8645 ). The Vandal produced an average of 5.30 more bots downed than the Sheriff. This value is significant (p-value = 0.0000004 ). The 95% Confidence Interval for this parameter is ( 3.1954, 7.4046 ). The Vandal produced an average of 4.85 more bots downed than the Phantom. This value is significant (p-value = 0.0000024 ). The 95% CI for this parameter is ( 2.7454, 6.9546 ). At long distance, the average of bots downed was 2.5667 less than at close distance. This value is significant (p-value = 0.0006838 ). The 95% CI for this parameter is ( 1.1368, 3.9965 ).","title":"Pairwise Comparisons (Tukey HSD)"},{"location":"valorant_aim/#conclusions","text":"Vandal performed best , significantly above Sheriff and Phantom. Sheriff and Phantom were similar , despite weapon differences. Accuracy loss at long distance was consistent across weapons. No significant performance improvement across rounds.","title":"Conclusions"},{"location":"valorant_aim/#disclaimer","text":"This dataset is from a single player (me). Results may not generalize, but the methodology can be reproduced. Shooting bots is not the same as competing against real players. All code is on GitHub . Clips of me playing are on YouTube .","title":"Disclaimer"}]}